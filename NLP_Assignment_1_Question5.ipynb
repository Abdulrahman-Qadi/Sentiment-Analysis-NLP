{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Optimising pre-processing and feature extraction (30 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abdulrahmanqadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abdulrahmanqadi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abdulrahmanqadi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv, nltk                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score # to report on precision and recall\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[0] == \"Id\":  # skip header\n",
    "                continue\n",
    "            (label, text) = parse_data_line(line)\n",
    "            raw_data.append((text, label))\n",
    "\n",
    "def split_and_preprocess_data(percentage):\n",
    "    # Modified to use new pre_process and to_feature_vector functions\n",
    "    num_samples = len(raw_data)\n",
    "    num_training_samples = int(percentage * num_samples)\n",
    "    for (text, label) in raw_data[:num_training_samples]:\n",
    "        train_data.append((to_feature_vector(pre_process(text)), label))\n",
    "    for (text, label) in raw_data[num_training_samples:]:\n",
    "        test_data.append((to_feature_vector(pre_process(text)), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_line(data_line):\n",
    "    # Should return a tuple of the label as just positive or negative and the statement\n",
    "    # e.g. (label, statement)\n",
    "    label = data_line[1] # sentiment label in the second column\n",
    "    statement = data_line[2] # Tweet in the third column\n",
    "    return (label, statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    # Converting text to lowercase (Normalisation)\n",
    "    text = text.lower()\n",
    "    # Tokenising the text (Tokenisation)\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing punctuation and stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    # Lemmatising the words (Lemmatisation)\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    tokens = [lemmatiser.lemmatize(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_dict = {}  # Global dictionary of features\n",
    "\n",
    "def to_feature_vector(tokens):\n",
    "    feature_vector = {}\n",
    "    \n",
    "    # Generating trigrams\n",
    "    n = 3\n",
    "    bigrams = nltk.ngrams(tokens, n)\n",
    "    all_tokens = tokens + [' '.join(bigram) for bigram in bigrams]\n",
    "    \n",
    "    # Calculating TF for each token\n",
    "    for token in all_tokens:\n",
    "        if token in feature_vector:\n",
    "            feature_vector[token] += 1\n",
    "        else:\n",
    "            feature_vector[token] = 1\n",
    "    \n",
    "    # Updating global feature dictionary\n",
    "    for token in feature_vector:\n",
    "        if token not in global_feature_dict:\n",
    "            global_feature_dict[token] = 1\n",
    "        else:\n",
    "            global_feature_dict[token] += 1\n",
    "\n",
    "\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "\n",
    "def train_classifier(data):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def cross_validate(dataset, folds):\n",
    "    # Initialising variables to store the sum of metrics across all folds\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    fold_size = int(len(dataset)/folds) + 1\n",
    "    \n",
    "    for i in range(0,len(dataset),int(fold_size)):\n",
    "        # inserting code here that trains and tests on the 10 folds of data in the dataset\n",
    "        print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
    "        # FILL IN THE METHOD HERE\n",
    "        \n",
    "        # Splitting the dataset into training and testing sets for this fold\n",
    "        train_data = dataset[:i] + dataset[i + fold_size:]\n",
    "        test_data = dataset[i:i + fold_size]\n",
    "        \n",
    "        # Splitting the data and labels for training and testing sets\n",
    "        train_data, train_labels = zip(*train_data)\n",
    "        test_data, test_labels = zip(*test_data)\n",
    "        \n",
    "        # Training the classifier\n",
    "        classifier = train_classifier(list(zip(train_data, train_labels)))\n",
    "        \n",
    "        # Predicting labels on the test set\n",
    "        predicted_labels = predict_labels(test_data, classifier)\n",
    "        \n",
    "        # Computing precision, recall, f1_score, and accuracy for this fold\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(test_labels, predicted_labels, average='weighted')\n",
    "        accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "        \n",
    "        # Summing the metrics for averaging later\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1_score += f1\n",
    "        total_accuracy += accuracy\n",
    "        \n",
    "    # Averaging the metrics over all folds\n",
    "    avg_precision = total_precision / folds\n",
    "    avg_recall = total_recall / folds\n",
    "    avg_f1_score = total_f1_score / folds\n",
    "    avg_accuracy = total_accuracy / folds\n",
    "        \n",
    "    # Creating a dictionary to hold the average metrics\n",
    "    cv_results = {\n",
    "        'precision': avg_precision,\n",
    "        'recall': avg_recall,\n",
    "        'f1_score': avg_f1_score,\n",
    "        'accuracy': avg_accuracy\n",
    "    }\n",
    "    \n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predict_labels(samples, classifier):\n",
    "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
    "    return classifier.classify_many(samples)\n",
    "\n",
    "def predict_label_from_raw(sample, classifier):\n",
    "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
    "    return classifier.classify(to_feature_vector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 33540 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 33540 rawData, 26832 trainData, 6708 testData\n",
      "Training Samples: \n",
      "26832\n",
      "Features: \n",
      "402838\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "# initialize global lists that will be appended to by the methods below\n",
    "raw_data = []          # the filtered data from the dataset file\n",
    "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
    "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
    "\n",
    "\n",
    "# references to the data files\n",
    "data_file_path = 'sentiment-dataset.tsv'\n",
    "\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "load_data(data_file_path) \n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "split_and_preprocess_data(0.8)\n",
    "\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
    "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold start on items 0 - 2684\n",
      "Training Classifier...\n",
      "Fold start on items 2684 - 5368\n",
      "Training Classifier...\n",
      "Fold start on items 5368 - 8052\n",
      "Training Classifier...\n",
      "Fold start on items 8052 - 10736\n",
      "Training Classifier...\n",
      "Fold start on items 10736 - 13420\n",
      "Training Classifier...\n",
      "Fold start on items 13420 - 16104\n",
      "Training Classifier...\n",
      "Fold start on items 16104 - 18788\n",
      "Training Classifier...\n",
      "Fold start on items 18788 - 21472\n",
      "Training Classifier...\n",
      "Fold start on items 21472 - 24156\n",
      "Training Classifier...\n",
      "Fold start on items 24156 - 26840\n",
      "Training Classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8598017186188786,\n",
       " 'recall': 0.8612860576655328,\n",
       " 'f1_score': 0.8589535494490322,\n",
       " 'accuracy': 0.8612860576655328}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'tomorrow': 1, \"'ll\": 2, 'release': 1, '58th': 1, 'episode': 1, 'hsonair': 1, 'profiling': 1, 'alissadossantos': 1, 'talk': 1, 'storytelling': 1, 'beyonce': 1, \"tomorrow 'll release\": 1, \"'ll release 58th\": 1, 'release 58th episode': 1, '58th episode hsonair': 1, 'episode hsonair profiling': 1, 'hsonair profiling alissadossantos': 1, \"profiling alissadossantos 'll\": 1, \"alissadossantos 'll talk\": 1, \"'ll talk storytelling\": 1, 'talk storytelling beyonce': 1}, 'positive')\n",
      "Training Classifier...\n",
      "Done training!\n",
      "Precision: 0.857849\n",
      "Recall: 0.859273\n",
      "F Score:0.857207\n"
     ]
    }
   ],
   "source": [
    "# Finally, check the accuracy of your classifier by training on all the traning data\n",
    "# and testing on the test set\n",
    "# Will only work once all functions are complete\n",
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    print(test_data[0])   # have a look at the first test data instance\n",
    "    classifier = train_classifier(train_data)  # train the classifier\n",
    "    test_true = [t[1] for t in test_data]   # get the ground-truth labels from the data\n",
    "    test_pred = predict_labels([x[0] for x in test_data], classifier)  #Â classify the test data to get predicted labels\n",
    "    final_scores = precision_recall_fscore_support(test_true, test_pred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % final_scores[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
